x <- get_all_tweets(query = "coin",
start_tweets = "2019-01-01T00:00:00Z",
end_tweets = "2020-03-01T00:00:00Z",
bearer_token = get_bearer())
x <- get_all_tweets(query = "coin",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = get_bearer())
bearer_token = "AAAAAAAAAAAAAAAAAAAAAJoXhwEAAAAAPNP0ebMlogKMnaRp8908OyONfos%3DUxX4JLViDgcsmQXKx3ux94vnCMfJpDZOrUdJF2kDjGdmiOblJY"
x <- get_all_tweets(query = "coin",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
API_key = "luI9uNEf9rZqmNTPO88yUHJFi"
API_secret = "mX2aRna4PgMpnv4JZmY72tKHgsKOFck4VgI2b1PqXw6r5SlS9V"
Access_token = "1113415761761009666-xitQBfXsNlKjLdEEreRV2L5zX4kuMO"
Access_secret = "lMoA7ww4YC6dbeAG2BpfAi50MaZHLifLypljwrDsi63lQ"
bearer_token = "AAAAAAAAAAAAAAAAAAAAAJoXhwEAAAAAPNP0ebMlogKMnaRp8908OyONfos%3DUxX4JLViDgcsmQXKx3ux94vnCMfJpDZOrUdJF2kDjGdmiOblJY"
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
Sys.setenv(BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAJoXhwEAAAAAEL3BzA3ofPsG1%2BoqZWfFfKcOYOE%3DUkEFoumS7lhK5z6lJWBvpFgNrOmW383QDhRhMoW8NWnnhdf2Hh")
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
API_key = "luI9uNEf9rZqmNTPO88yUHJFi"
API_secret = "mX2aRna4PgMpnv4JZmY72tKHgsKOFck4VgI2b1PqXw6r5SlS9V"
Access_token = "1113415761761009666-xitQBfXsNlKjLdEEreRV2L5zX4kuMO"
Access_secret = "lMoA7ww4YC6dbeAG2BpfAi50MaZHLifLypljwrDsi63lQ"
bearer_token = "AAAAAAAAAAAAAAAAAAAAAJoXhwEAAAAAPNP0ebMlogKMnaRp8908OyONfos%3DUxX4JLViDgcsmQXKx3ux94vnCMfJpDZOrUdJF2kDjGdmiOblJY"
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
library(twitteR)
library(academictwitteR)
library(tidyverse)
API_key = "luI9uNEf9rZqmNTPO88yUHJFi"
API_secret = "mX2aRna4PgMpnv4JZmY72tKHgsKOFck4VgI2b1PqXw6r5SlS9V"
Access_token = "1113415761761009666-xitQBfXsNlKjLdEEreRV2L5zX4kuMO"
Access_secret = "lMoA7ww4YC6dbeAG2BpfAi50MaZHLifLypljwrDsi63lQ"
bearer_token = "AAAAAAAAAAAAAAAAAAAAAJoXhwEAAAAAPNP0ebMlogKMnaRp8908OyONfos%3DUxX4JLViDgcsmQXKx3ux94vnCMfJpDZOrUdJF2kDjGdmiOblJY"
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
library(twitteR)
library(academictwitteR)
library(tidyverse)
API_key = "luI9uNEf9rZqmNTPO88yUHJFi"
API_secret = "mX2aRna4PgMpnv4JZmY72tKHgsKOFck4VgI2b1PqXw6r5SlS9V"
Access_token = "1113415761761009666-xitQBfXsNlKjLdEEreRV2L5zX4kuMO"
Access_secret = "lMoA7ww4YC6dbeAG2BpfAi50MaZHLifLypljwrDsi63lQ"
bearer_token = "AAAAAAAAAAAAAAAAAAAAAJoXhwEAAAAAPNP0ebMlogKMnaRp8908OyONfos%3DUxX4JLViDgcsmQXKx3ux94vnCMfJpDZOrUdJF2kDjGdmiOblJY"
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "math",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "crypto",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "twitter",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "coins",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "coin",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "coin",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
x <- get_all_tweets(query = "coin",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
API_key = "RYwLw5mp7BPFwCQ5WtQ8S3543"
API_secret = "puMpq7cj9KdIZyusLwCOwdUqkvq5ytDYZSWMRLz7X0wZ84hm8g"
Access_token = "1113415761761009666-91jLmFTQ6y5PKy8o4pRjJz6UXLgTlb"
Access_secret = "w5Is4kQnWU46mMBpUkQNlObyBoVrjIKakgGLfDhfebYtn"
bearer_token = "AAAAAAAAAAAAAAAAAAAAACAreAEAAAAAvFHashRDshTxFe2%2Bu7uAvd%2BYOiI%3DI6ptSQog0kkUy2lkNgsFN5C7aQ1M3pqB97hJVJA9SpSQtxNCjL"
x <- get_all_tweets(query = "coin",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token)
View(x)
scrapedData <- get_all_tweets(query = "blockchain",
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token)
View(scrapedData)
View(scrapedData)
View(scrapedData)
keyword <- c("AI", "Artificial Intelligence", "Machine Learning", "ML")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token)
View(scrapedData)
keyword <- c("Artificial Intelligence", "Machine Learning")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token)
View(scrapedData)
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 200)
View(scrapedData)
keyword <- c("Artificial Intelligence", "Machine Learning", "AI", "ML")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 200,
exact_phrase = T,
country = "US",
lang = "en")
keyword <- c("Artificial Intelligence", "Machine Learning")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 200,
#exact_phrase = T,
country = "US",
lang = "en")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 500,
#exact_phrase = T,
country = "US",
lang = "en")
View(scrapedData)
keyword <- ("Artificial Intelligence" & "Automotive")
keyword <- ("Artificial Intelligence Automotive")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
keyword <- c("Artificial Intelligence", "Automotive")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
View(scrapedData)
scrapedData <- get_all_tweets(query = "Artificial Intelligence" & "Automotive",
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
scrapedData <- get_all_tweets(query = str_detect("Artificial Intelligence") & str_detect("automotive"),
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
keyword <- c("Artificial Intelligence Automotive")
scrapedData <- get_all_tweets(query = str_detect("Artificial Intelligence") & str_detect("automotive"),
start_tweets = "2019-01-01T00:00:00Z",
end_tweets = "2022-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
keyword <- c("Artificial Intelligence Automotive")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2019-01-01T00:00:00Z",
end_tweets = "2022-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
View(scrapedData)
keyword <- c("Artificial Intelligence Healthcare")
scrapedData <- get_all_tweets(query = keyword,
start_tweets = "2019-01-01T00:00:00Z",
end_tweets = "2022-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 5,
#exact_phrase = T,
country = "US",
lang = "en")
healthcareQuery <- c("Artificial Intelligence Healthcare", "Artificial Intelligence Health Services", "Artificial Intelligence Health Providers", "Artificial Intelligence Pharma", "Artificial Intelligence Life Science", "Artificial Intelligence Insurance", "Artificial Intelligence Consumer Health")
install.packages("rjson")
install.packages("rjson")
library(rjson)
library(twitteR)
library(academictwitteR)
library(tidyverse)
bearer_token = "AAAAAAAAAAAAAAAAAAAAACAreAEAAAAAvFHashRDshTxFe2%2Bu7uAvd%2BYOiI%3DI6ptSQog0kkUy2lkNgsFN5C7aQ1M3pqB97hJVJA9SpSQtxNCjL"
healthcareQuery <- c("Artificial Intelligence Healthcare", "Artificial Intelligence Health Services", "Artificial Intelligence Health Providers", "Artificial Intelligence Pharma", "Artificial Intelligence Life Science", "Artificial Intelligence Insurance", "Artificial Intelligence Consumer Health")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en")
write_json(healthcareData, healthcareData.json, pretty = TRUE, na = TRUE, auto_unbox = FALSE)
library(rjson)
write_json(healthcareData, healthcareData.json, pretty = TRUE, na = TRUE, auto_unbox = FALSE)
install.packages("jsonlite")
install.packages("jsonlite")
install.packages("jsonlite")
library(jsonlite)
bearer_token = "AAAAAAAAAAAAAAAAAAAAACAreAEAAAAAvFHashRDshTxFe2%2Bu7uAvd%2BYOiI%3DI6ptSQog0kkUy2lkNgsFN5C7aQ1M3pqB97hJVJA9SpSQtxNCjL"
healthcareQuery <- c("Artificial Intelligence Healthcare", "Artificial Intelligence Health Services", "Artificial Intelligence Health Providers", "Artificial Intelligence Pharma", "Artificial Intelligence Life Science", "Artificial Intelligence Insurance", "Artificial Intelligence Consumer Health")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en")
library(twitteR)
library(academictwitteR)
library(tidyverse)
library(rjson)
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en")
write_json(healthcareData, healthcareData.json, pretty = TRUE, na = TRUE, auto_unbox = FALSE)
?write_json
View(healthcareData)
write_json(healthcareData, healthcareData.json)
write_json(healthcareData, "healthcareData.json")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics.retweet_count", "public_metrics.like_count", "public_metrics.quote_count")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics.retweet_count", "public_metrics.like_count", "public_metrics.quote_count")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics.retweet_count", "public_metrics.like_count", "public_metrics.quote_count")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics retweet_count", "public_metrics.like_count", "public_metrics.quote_count")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metricsretweet_count", "public_metrics.like_count", "public_metrics.quote_count")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics.like_count", "public_metrics.quote_count")
colnames(healthcareData)
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-01-05T00:00:00Z",
bearer_token = bearer_token,
n = 2,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
setwd("~/Desktop/Twitter_NLP/applicationsForAIML")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-02-01T00:00:00Z",
bearer_token = bearer_token,
n = Inf,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(healthcareData)
?get_all_tweets
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-02-01T00:00:00Z",
bearer_token = bearer_token,
n = Inf,
country = "US",
data_path = "healthcareData.json",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-02-01T00:00:00Z",
bearer_token = bearer_token,
n = Inf,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
write_json(healthcareData, "healthcareData.json")
#### Automotive ####
automotiveQuery <- c("Artificial Intelligence Automotive", "Artificial Intelligence Component Suppliers", "Artificial Intelligence OEM", "Artificial Intelligence Original Equipment Manufacturer")
#### Automotive ####
automotiveQuery <- c("Artificial Intelligence Automotive", "Artificial Intelligence Component Suppliers", "Artificial Intelligence OEM", "Artificial Intelligence Original Equipment Manufacturer")
automotiveData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-02-01T00:00:00Z",
bearer_token = bearer_token,
n = Inf,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
write_json(automotiveData, "automotiveData.json")
automotiveData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-02-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(automotiveData)
automotiveData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-03-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
automotiveData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-03-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
automotiveData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-03-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-03-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2021-03-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
colnames(automotiveData)
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(automotiveData)
#### Automotive ####
automotiveQuery <- c("Artificial Intelligence Automotive", "Artificial Intelligence Component Suppliers", "Artificial Intelligence OEM", "Artificial Intelligence Original Equipment Manufacturer", "Artificial Intelligence Car")
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(automotiveData)
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2019-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(automotiveData)
#### Automotive ####
financeQuery <- c("Artificial Intelligence Financial Services", "Artificial Intelligence Wealth Management", "Artificial Intelligence Banking")
#### Automotive ####
financeQuery <- c("Artificial Intelligence Financial Services", "Artificial Intelligence Wealth Management", "Artificial Intelligence Banking")
financeData <- get_all_tweets(query = financeQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
#### Automotive ####
financeQuery <- c("Artificial Intelligence Financial Services", "Artificial Intelligence Wealth Management", "Artificial Intelligence Banking", "Artificial Intelligence Finance")
financeData <- get_all_tweets(query = financeQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(financeData)
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
country = "US") %>% select("created_at", "text", "public_metrics", "lang")
automotiveData <- get_all_tweets(query = automotiveQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-01-01T00:00:00Z",
bearer_token = bearer_token,
n = 10000,
#country = "US",
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
#### Retail ####
retailQuery <- c("Artificial Intelligence Retail", "Artificial Intelligence Consumer Retail")
retailData <- get_all_tweets(query = retailQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-09-01T00:00:00Z",
bearer_token = bearer_token,
n = 1000,
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
retailData <- get_all_tweets(query = retailQuery,
start_tweets = "2018-01-01T00:00:00Z",
end_tweets = "2022-09-01T00:00:00Z",
bearer_token = bearer_token,
n = 1000,
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(retailData)
#### Healthcare ####
healthcareQuery <- c("Artificial Intelligence Healthcare", "Artificial Intelligence Health Services", "Artificial Intelligence Health Providers", "Artificial Intelligence Pharma", "Artificial Intelligence Life Science", "Artificial Intelligence Insurance", "Artificial Intelligence Consumer Health")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2019-01-01T00:00:00Z",
end_tweets = "2022-09-01T00:00:00Z",
bearer_token = bearer_token,
n = Inf,
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2022-01-01T00:00:00Z",
end_tweets = "2022-05-01T00:00:00Z",
bearer_token = bearer_token,
n = Inf,
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
#### Healthcare ####
healthcareQuery <- c("Artificial Intelligence Healthcare", "Artificial Intelligence Health Services", "Artificial Intelligence Health Providers", "Artificial Intelligence Pharma", "Artificial Intelligence Life Science", "Artificial Intelligence Insurance", "Artificial Intelligence Consumer Health")
healthcareData <- get_all_tweets(query = healthcareQuery,
start_tweets = "2022-01-01T00:00:00Z",
end_tweets = "2022-03-01T00:00:00Z",
is_retweet = FALSE,
bearer_token = bearer_token,
n = Inf,
lang = "en") %>% select("created_at", "text", "public_metrics", "lang")
View(healthcareData)
